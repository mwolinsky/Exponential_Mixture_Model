{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Mixture Model\n",
    "\n",
    "En esta notebook se realizará un modelo similar al Guassian Mixture Model (de aquí en más GMM) pero utilizando en vez de una distribución gaussiana, una exponencial. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialMixtureModel():\n",
    "    def __init__(self, k: int, max_iter: int = 100, tol: float = 1e-6):\n",
    "        #Cantidad de Clusters\n",
    "        self.k = k\n",
    "        #Maxima cantidad de iteraciones del algirtmo EM\n",
    "        self.max_iter = max_iter\n",
    "        #Tolerancia para comprar log-likelihood\n",
    "        self.tol = tol\n",
    "        #Pesos /pi's\n",
    "        self.weights = None\n",
    "        #Parametro lambda de la exponencial\n",
    "        self.lambdas = None\n",
    "\n",
    "    def _prepare_data(self, X: np.ndarray | pd.DataFrame) -> np.ndarray:\n",
    "        '''\n",
    "        Prepara los datos de entrada para el algoritmo.\n",
    "\n",
    "        Args:\n",
    "        X (array-like): Datos de entrada.\n",
    "        '''\n",
    "        if not isinstance(X, (np.ndarray, pd.DataFrame)):\n",
    "            raise ValueError(\"X debe ser numpy.ndarray o un pandas.DataFrame.\")\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.values\n",
    "        elif X.ndim == 1:\n",
    "            return X.reshape(-1, 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X: np.ndarray | pd.DataFrame) -> None:\n",
    "        '''\n",
    "        Inicializa los parámetros del modelo.\n",
    "\n",
    "        Args:\n",
    "        X (array-like): Datos de entrada.\n",
    "        '''\n",
    "        # Preparar los datos\n",
    "        X = self._prepare_data(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Se asignan los pesos uniformemente\n",
    "        self.weights = np.full(self.k, 1 / self.k)\n",
    "        \n",
    "        # Se inicializan los cuantiles entre valores de 0.2 y 0.8 para cada clase\n",
    "        quantiles = np.quantile(X, q=np.linspace(0.2, 0.8, self.k), axis=0)\n",
    "        self.lambdas = 1 / (quantiles + 1e-6)  # 1/ quantiles para \"estimar un posible lambda mejor que lo uniforme\"\n",
    "        \n",
    "    def e_step(self, X: np.ndarray | pd.DataFrame) -> None:\n",
    "        '''\n",
    "        Computa el paso E del algoritmo EM.\n",
    "\n",
    "        Args:\n",
    "        X (array-like): Datos de entrada.\n",
    "        '''\n",
    "\n",
    "        # En este paso se calcula la probabilidad de pertenecer al cluster j dado las observaciones, ponderándolas \n",
    "        # por el peso de cada cluster inicializado anteriormente. Es una regla de bayes ponderada con la exponencial como densidad.\n",
    "        X = self._prepare_data(X)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "        self.e_return = np.zeros((n_samples, self.k))\n",
    "        for k in range(self.k):\n",
    "            # Calcular la probabilidad de pertenencia de cada observacion a cada cluster\n",
    "            probs = self.weights[k] * np.prod(self.lambdas[k] * np.exp(-self.lambdas[k] * X), axis=1)\n",
    "\n",
    "            print(f'Probs for cluster {k}', probs)\n",
    "            \n",
    "            # Evitar ceros en las probabilidades\n",
    "            probs = np.where(probs < 1e-10, 1e-10, probs)\n",
    "            self.e_return[:, k] = probs\n",
    "        # Normalizar las probabilidades para que sumen 1 por observación\n",
    "        self.e_return = self.e_return / self.e_return.sum(axis=1, keepdims=True)\n",
    "        if np.any(np.isnan(self.e_return)):\n",
    "            raise ValueError(\"Advertencia: NaNs encontrados en la matriz de probabilidad después del paso E.\")\n",
    "        \n",
    "\n",
    "    def m_step(self, X):\n",
    "        X = self._prepare_data(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        old_lambdas = self.lambdas.copy()\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            resp_sum = np.sum(self.e_return[:, k])\n",
    "            self.weights[k] = resp_sum / n_samples\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                weighted_sum = np.sum(self.e_return[:, k] * X[:, j])\n",
    "                if weighted_sum < 1e-10:\n",
    "                    self.lambdas[k, j] = old_lambdas[k, j]  # Mantener valor anterior\n",
    "                else:\n",
    "                    # Limitar el cambio en lambda para evitar convergencia rápida a un solo cluster\n",
    "                    self.lambdas[k, j] = resp_sum / weighted_sum\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = self._prepare_data(X)\n",
    "        self.initialize_parameters(X)\n",
    "        log_likelihood_old = -np.inf\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            self.e_step(X)\n",
    "            self.m_step(X)\n",
    "            \n",
    "            # Calcular log-likelihood\n",
    "            #log_probs = np.sum(np.log(np.sum(self.e_return, axis=1) + 1e-300))\n",
    "            log_probs=0\n",
    "            for i in range(X.shape[0]):  # Iteramos sobre cada observación\n",
    "                total_density = 0\n",
    "                for k in range(self.k):  # Iteramos sobre cada componente\n",
    "                    # Calculamos la densidad para el componente k, ponderada por su peso\n",
    "                    density = self.weights[k] * np.prod(self.lambdas[k] * np.exp(-self.lambdas[k] * X[i]))\n",
    "                    total_density += density\n",
    "                # Sumamos el logaritmo de la densidad total de la observación i al log-likelihood\n",
    "                log_probs += np.log(total_density + 1e-300)  # Evitar log(0)\n",
    "\n",
    "            print(\"log_probs\",log_probs)\n",
    "            print(\"np.sum(self.e_return, axis=1):\",np.sum(self.e_return, axis=1))\n",
    "\n",
    "            # Verificar si los clusters están balanceados\n",
    "            cluster_weights = np.mean(self.e_return, axis=0)\n",
    "            max_weight_ratio = np.max(cluster_weights) / np.min(cluster_weights)\n",
    "            \n",
    "            print(f\"Log-likelihood: {log_probs}\")\n",
    "            print(f\"Ratio max/min de pesos de clusters: {max_weight_ratio}\")\n",
    "            \n",
    "            if i > 0 and np.abs(log_probs - log_likelihood_old) < self.tol:\n",
    "                if max_weight_ratio > 100:  # Si los clusters están muy desbalanceados\n",
    "                    print(\"Reiniciando debido a clusters desbalanceados...\")\n",
    "                    self.initialize_parameters(X)\n",
    "                    log_likelihood_old = -np.inf\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"Convergencia alcanzada en la iteración {i+1}\")\n",
    "                break\n",
    "                \n",
    "            log_likelihood_old = log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self._prepare_data(X)\n",
    "        self.e_step(X)\n",
    "        return self.e_return\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos iniciales:\n",
      " [0.5 0.5]\n",
      "Parámetros iniciales (lambdas):\n",
      " [[3.80616294 7.2670914 ]\n",
      " [4.39976655 2.6164331 ]]\n",
      "Iteración 0, log-likelihood: 9.999997274190034e-09\n",
      "Convergencia alcanzada en la iteración 1.\n",
      "Pesos de mezcla:\n",
      " [0.54749549 0.45250451]\n",
      "Tasas (lambda):\n",
      " [[1.83040555 6.34048387]\n",
      " [2.82439975 3.65110362]]\n",
      "\n",
      "Asignación de clusters: [0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      "Matriz de probabilidad: [[0.52374571 0.47625429]\n",
      " [0.68214273 0.31785727]\n",
      " [0.51321092 0.48678908]\n",
      " [0.25699224 0.74300776]\n",
      " [0.60557501 0.39442499]\n",
      " [0.58976087 0.41023913]\n",
      " [0.52809547 0.47190453]\n",
      " [0.72252746 0.27747254]\n",
      " [0.8606207  0.1393793 ]\n",
      " [0.25111459 0.74888541]\n",
      " [0.6834565  0.3165435 ]\n",
      " [0.41948483 0.58051517]\n",
      " [0.51980567 0.48019433]\n",
      " [0.80379236 0.19620764]\n",
      " [0.36382919 0.63617081]\n",
      " [0.52058662 0.47941338]\n",
      " [0.30437897 0.69562103]\n",
      " [0.67455497 0.32544503]\n",
      " [0.47722078 0.52277922]\n",
      " [0.66560918 0.33439082]\n",
      " [0.82125902 0.17874098]\n",
      " [0.67530306 0.32469694]\n",
      " [0.25639618 0.74360382]\n",
      " [0.62408441 0.37591559]\n",
      " [0.5186664  0.4813336 ]\n",
      " [0.57806994 0.42193006]\n",
      " [0.5927149  0.4072851 ]\n",
      " [0.82551721 0.17448279]\n",
      " [0.52370255 0.47629745]\n",
      " [0.59643249 0.40356751]\n",
      " [0.48595438 0.51404562]\n",
      " [0.67856665 0.32143335]\n",
      " [0.63021406 0.36978594]\n",
      " [0.63084394 0.36915606]\n",
      " [0.46611296 0.53388704]\n",
      " [0.57586265 0.42413735]\n",
      " [0.57932166 0.42067834]\n",
      " [0.55379116 0.44620884]\n",
      " [0.76334795 0.23665205]\n",
      " [0.63972345 0.36027655]\n",
      " [0.33401112 0.66598888]\n",
      " [0.58608993 0.41391007]\n",
      " [0.64457399 0.35542601]\n",
      " [0.29793861 0.70206139]\n",
      " [0.49307059 0.50692941]\n",
      " [0.5513389  0.4486611 ]\n",
      " [0.59129256 0.40870744]\n",
      " [0.27334087 0.72665913]\n",
      " [0.45594434 0.54405566]\n",
      " [0.04287989 0.95712011]\n",
      " [0.65505538 0.34494462]\n",
      " [0.37894336 0.62105664]\n",
      " [0.91887932 0.08112068]\n",
      " [0.46207963 0.53792037]\n",
      " [0.5876048  0.4123952 ]\n",
      " [0.35042584 0.64957416]\n",
      " [0.487299   0.512701  ]\n",
      " [0.50026484 0.49973516]\n",
      " [0.58408599 0.41591401]\n",
      " [0.60096035 0.39903965]\n",
      " [0.43825917 0.56174083]\n",
      " [0.51045155 0.48954845]\n",
      " [0.53765146 0.46234854]\n",
      " [0.33181469 0.66818531]\n",
      " [0.17108005 0.82891995]\n",
      " [0.37653324 0.62346676]\n",
      " [0.76085736 0.23914264]\n",
      " [0.52982973 0.47017027]\n",
      " [0.6245552  0.3754448 ]\n",
      " [0.56406059 0.43593941]\n",
      " [0.85520649 0.14479351]\n",
      " [0.64403991 0.35596009]\n",
      " [0.88679058 0.11320942]\n",
      " [0.68137958 0.31862042]\n",
      " [0.53191141 0.46808859]\n",
      " [0.54791185 0.45208815]\n",
      " [0.56120544 0.43879456]\n",
      " [0.26048682 0.73951318]\n",
      " [0.45703017 0.54296983]\n",
      " [0.58761985 0.41238015]\n",
      " [0.59919938 0.40080062]\n",
      " [0.51300277 0.48699723]\n",
      " [0.46968801 0.53031199]\n",
      " [0.6788853  0.3211147 ]\n",
      " [0.32311651 0.67688349]\n",
      " [0.48751226 0.51248774]\n",
      " [0.56568936 0.43431064]\n",
      " [0.46961287 0.53038713]\n",
      " [0.50764771 0.49235229]\n",
      " [0.80601034 0.19398966]\n",
      " [0.55630606 0.44369394]\n",
      " [0.67462047 0.32537953]\n",
      " [0.56663746 0.43336254]\n",
      " [0.34999588 0.65000412]\n",
      " [0.43902863 0.56097137]\n",
      " [0.51165573 0.48834427]\n",
      " [0.64768013 0.35231987]\n",
      " [0.54015043 0.45984957]\n",
      " [0.76026579 0.23973421]\n",
      " [0.50113217 0.49886783]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# X = np.concatenate([\n",
    "#     np.random.exponential(scale=1/2, size=100),  \n",
    "#     np.random.exponential(scale=1/5, size=100) \n",
    "# ])\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'X1': np.random.exponential(scale=1/2, size=100),  \n",
    "    'X2': np.random.exponential(scale=1/5, size=100)})\n",
    "\n",
    "# Crear y ajustar el modelo\n",
    "emm = ExponentialMixtureModel(k=2, max_iter=1000)\n",
    "emm.fit(X)\n",
    "\n",
    "# Predicciones\n",
    "print(\"Pesos de mezcla:\\n\", emm.weights)\n",
    "print(\"Tasas (lambda):\\n\", emm.lambdas)\n",
    "\n",
    "clusters = emm.predict(X)\n",
    "print(\"\\nAsignación de clusters:\", clusters)\n",
    "print(\"Matriz de probabilidad:\",emm.predict_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([\n",
    "    np.random.exponential(scale=1/2, size=100),  \n",
    "    np.random.exponential(scale=1/5, size=100)\n",
    "])\n",
    "\n",
    "emm = ExponentialMixtureModel(k=2, max_iter=10)\n",
    "emm.initialize_parameters(X)\n",
    "# emm.e_step(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emm.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0.5\n",
      "Lambda: [15.5674366]\n",
      "Shape Exponential: (200, 1)\n",
      "Shape Product: [6.58391769e-06 1.02564251e-04 2.19355821e+00 2.20534289e-01\n",
      " 1.42526997e-01 2.10854696e-05 4.77976875e-02 1.95936370e+00\n",
      " 5.63152013e-08 1.35207663e-03 6.09620066e-04 1.22979436e+00\n",
      " 1.39316410e+01 4.81501805e-04 4.09292431e+00 2.13882638e-07\n",
      " 2.27344949e-02 2.64476307e-03 2.16436614e-01 8.45562150e-02\n",
      " 2.76258704e-05 3.83936666e+00 1.44134926e+01 3.78608441e-04\n",
      " 5.72817754e-04 6.05119697e+00 3.50609114e-01 3.25002385e-05\n",
      " 1.33360684e+01 6.46602239e+00 1.44043431e+00 5.40186086e+00\n",
      " 9.53042577e-06 2.54996789e-01 4.29441283e-05 3.52048539e-10\n",
      " 7.48343390e-08 3.73632768e-01 6.89636737e-03 4.10621020e-02\n",
      " 1.38711813e-06 3.64513015e+00 2.03034213e-04 1.34739224e+01\n",
      " 1.10613046e-01 2.88346613e-02 7.06886206e-03 1.33178725e-03\n",
      " 2.03401490e-04 2.82474850e-07 7.91269159e-01 1.27611551e+01\n",
      " 1.07845666e+00 1.03112274e+01 6.92804297e-06 6.99510679e-02\n",
      " 1.16478372e-01 1.03414956e+01 4.95798132e-04 7.90478992e-03\n",
      " 1.13725578e-01 3.03247460e-01 4.94424088e-04 6.40068806e-09\n",
      " 2.00849159e-02 4.46053776e-07 7.62858124e-04 2.27199753e-02\n",
      " 3.97475737e-03 3.92547436e-01 1.55833789e+00 8.29009610e-03\n",
      " 1.08793278e+01 1.38085739e+01 1.37862319e-01 5.87819766e-02\n",
      " 2.46112312e-04 5.96686567e-04 1.43363367e-06 3.03930112e-04\n",
      " 5.42474485e-01 7.46219780e-01 5.21225761e-01 1.72142833e-01\n",
      " 4.42526303e-02 4.91476048e+00 9.89855104e+00 1.19031396e+01\n",
      " 6.48119618e-01 5.80428677e-01 3.20241245e-02 6.24221673e-03\n",
      " 1.20157527e+01 7.59848024e-02 3.12674120e-05 3.26243532e-11\n",
      " 4.58271638e+00 3.01417232e-02 9.09809471e+00 4.05929203e-05\n",
      " 9.62987499e+00 6.13370217e-01 1.31671866e+00 2.73570729e+00\n",
      " 1.03488158e+01 1.71179100e-06 2.96680593e-03 1.29439453e-04\n",
      " 7.69918586e+00 2.71229533e-01 8.38427440e-02 1.04682309e+00\n",
      " 1.43590126e+01 3.04168097e+00 6.45345194e+00 5.64873770e+00\n",
      " 9.45177705e-01 2.02115793e-02 1.32817241e+01 1.07675591e-01\n",
      " 1.68778327e-01 5.78812990e-02 4.37153971e-06 3.58818995e-01\n",
      " 2.85394227e-01 5.27176336e-01 4.56151224e-03 3.72760108e+00\n",
      " 1.95548132e-01 4.01916817e-02 9.72171132e-04 2.84401748e+00\n",
      " 7.68291253e-03 7.70779128e+00 2.45882927e+00 6.74189480e+00\n",
      " 8.66115900e+00 9.84277577e-02 3.84842028e-02 3.25515540e+00\n",
      " 1.11569120e-01 9.49102863e+00 2.63004184e+00 2.28159557e+00\n",
      " 3.55593656e-01 6.61146901e+00 6.39929964e-04 3.40719046e-01\n",
      " 1.50986291e+01 1.32064460e-04 1.11533266e+00 1.06003105e+01\n",
      " 6.08659807e+00 2.12986459e+00 1.15475864e+01 2.79412174e-02\n",
      " 2.09650626e+00 1.23885933e+00 1.08647833e+00 2.44158139e+00\n",
      " 5.28486710e+00 1.61113814e-01 1.04385411e+01 2.00677212e+00\n",
      " 2.35839757e+00 1.18364312e-01 5.28009215e+00 2.59562097e-05\n",
      " 1.47252724e+01 5.94160134e-01 3.95701828e-03 1.50572187e+01\n",
      " 1.49831989e+01 4.66008719e+00 4.08189801e-03 7.15464363e-02\n",
      " 1.86854080e+00 4.94597734e+00 9.33486580e+00 2.91064178e+00\n",
      " 5.82691489e-04 2.09586684e+00 3.52942011e+00 1.21663345e+01\n",
      " 3.52447091e+00 1.50703708e+00 2.29502792e-03 7.60106642e+00\n",
      " 1.01760147e+00 8.83578745e+00 1.40108073e-01 8.92164537e+00\n",
      " 3.61931502e-04 8.27030437e+00 3.02626416e-01 2.09246936e-01\n",
      " 6.74770407e-03 5.28403754e-02 2.74137600e+00 1.15016922e+00]\n",
      "Weight: 0.5\n",
      "Lambda: [1.59999676]\n",
      "Shape Exponential: (200, 1)\n",
      "Shape Product: [0.35402737 0.46946294 1.30812026 1.03301922 0.98769708 0.39901683\n",
      " 0.88278796 1.29302828 0.21702289 0.61194685 0.56384316 1.23258744\n",
      " 1.58184403 0.55033547 1.39472557 0.2489263  0.81787623 0.65563457\n",
      " 1.03102983 0.93609213 0.41025179 1.38558822 1.58738175 0.53690401\n",
      " 0.56024618 1.45191543 1.08343419 0.41716093 1.5747568  1.46184363\n",
      " 1.25277941 1.43507487 0.36774427 1.04855077 0.42928092 0.12881888\n",
      " 0.22345812 1.09053964 0.72350524 0.86911361 0.30166192 1.37821472\n",
      " 0.50359643 1.57642214 0.96229629 0.83810292 0.72534464 0.61099663\n",
      " 0.50368998 0.25614557 1.1779716  1.56764073 1.21606365 1.53366798\n",
      " 0.35588602 0.91802534 0.96741997 1.53413008 0.55199292 0.73372507\n",
      " 0.9650448  1.06739416 0.55183549 0.17355683 0.80752604 0.26845955\n",
      " 0.57698875 0.81782253 0.68366886 1.09608887 1.26295058 0.73732287\n",
      " 1.54214508 1.58040214 0.98432487 0.90175745 0.51365466 0.56260184\n",
      " 0.3026863  0.52491609 1.13314326 1.17089605 1.12849921 1.00704936\n",
      " 0.87582358 1.42120402 1.52724316 1.55646623 1.1540566  1.14104658\n",
      " 0.84718877 0.71613232 1.5579733  0.92586516 0.41550619 0.10087955\n",
      " 1.41102296 0.84193036 1.51406431 0.42680382 1.52292981 1.14753877\n",
      " 1.24126983 1.33815441 1.53424165 0.30825348 0.66342332 0.48082739\n",
      " 1.48830625 1.05522278 0.93527724 1.2123484  1.58676404 1.35281547\n",
      " 1.46155128 1.44168136 1.19968771 0.80804797 1.57409605 0.95963797\n",
      " 1.00500845 0.9003275  0.33943584 1.08601466 1.06075822 1.12981662\n",
      " 0.69341266 1.38138749 1.02033094 0.86720185 0.59154775 1.34350517\n",
      " 0.73158124 1.48847713 1.32355913 1.46813438 1.50642492 0.95082173\n",
      " 0.86334116 1.36227948 0.96314785 1.52065826 1.33274788 1.31342145\n",
      " 1.08500728 1.46518963 0.56666213 1.08025263 1.59497635 0.48182058\n",
      " 1.22027313 1.53803258 1.45278615 1.30416458 1.55162254 0.83539606\n",
      " 1.30205032 1.23351817 1.21699021 1.32260188 1.43184895 1.00021933\n",
      " 1.53560353 1.29620942 1.31789828 0.9690183  1.43171593 0.40763155\n",
      " 1.59087705 1.143792   0.68335464 1.5945262  1.59371878 1.41345306\n",
      " 0.68554039 0.92015554 1.28673616 1.42212916 1.51806751 1.34670643\n",
      " 0.56123112 1.3020095  1.37365284 1.55996881 1.37345474 1.25861295\n",
      " 0.64614623 1.4863456  1.20882581 1.50951872 0.98596095 1.51101975\n",
      " 0.53442394 1.49929233 1.06716928 1.02745617 0.72188655 0.89193525\n",
      " 1.33843913 1.22413663]\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(-1, 1)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "e_return = np.zeros((n_samples, 2))\n",
    "for k in range(2):\n",
    "    print('Weight:', emm.weights[k])\n",
    "    print('Lambda:', emm.lambdas[k])\n",
    "    print('Shape Exponential:', np.exp(-emm.lambdas[k] * X).shape)\n",
    "    print('Shape Product:', np.prod(emm.lambdas[k] * np.exp(-emm.lambdas[k] * X), axis=1))\n",
    "    probs = emm.weights[k] * np.prod(emm.lambdas[k] * np.exp(-emm.lambdas[k] * X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
